{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a2dfe0",
   "metadata": {},
   "source": [
    "# Coding the Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952f42f",
   "metadata": {},
   "source": [
    "## Attending to different parts of the input with self-attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a967e9d",
   "metadata": {},
   "source": [
    "### A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5f25ff99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [\n",
    "    [0.43, 0.15, 0.89], # Your   (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts  (x^3)\n",
    "    [0.22, 0.58, 0.33], # with    (x^4)\n",
    "    [0.77, 0.25, 0.10], # one     (x^5)\n",
    "    [0.05, 0.80, 0.55]  # step    (x^6)\n",
    "  ]\n",
    ")\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d70a15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4e009ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query = inputs[1]\n",
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "385cf63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = inputs[0]\n",
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "db22ff70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9544"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.55 * 0.43 + 0.87 * 0.15 + 0.66 * 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3db5234e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(input_query, input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b0a1ed00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8434)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = 0.\n",
    "i = 3 # index of the input vector\n",
    "\n",
    "for idx, element in enumerate(inputs[i]):\n",
    "  res += element * input_query[idx]\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2591387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8434)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the same as above but using dot product\n",
    "\n",
    "i = 3 # index of the input vector\n",
    "\n",
    "res = torch.dot(inputs[i], input_query)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "643da429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "07778523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now do them all at once in a loop\n",
    "\n",
    "query = inputs[1]\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "  attn_scores_2[i] = torch.dot(x_i, query) # dot product of x_i and query\n",
    "\n",
    "attn_scores_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950950f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the scores but with a simplified formula (no softmax)\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "attn_weights_2_tmp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fe167134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2_tmp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e45295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now do it with a softmax\n",
    "\n",
    "def softmax_naive(x):\n",
    "  return torch.exp(x) / torch.exp(x).sum()\n",
    "\n",
    "softmax_naive(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eba63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now with a softmax from pytorch (it is recommended to use this over the naive implementation)\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(attn_weights_2)\n",
    "\n",
    "attn_weights_2.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b9da6",
   "metadata": {},
   "source": [
    "#### now calculate the context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2c32ef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13854756951332092 ---> tensor([0.4300, 0.1500, 0.8900])\n",
      "0.2378913015127182 ---> tensor([0.5500, 0.8700, 0.6600])\n",
      "0.23327402770519257 ---> tensor([0.5700, 0.8500, 0.6400])\n",
      "0.12399158626794815 ---> tensor([0.2200, 0.5800, 0.3300])\n",
      "0.10818186402320862 ---> tensor([0.7700, 0.2500, 0.1000])\n",
      "0.15811361372470856 ---> tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1] # the second input vector is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "  print(f\"{attn_weights_2[i]} ---> {x_i}\")\n",
    "  context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "context_vec_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138cecc3",
   "metadata": {},
   "source": [
    "## A simple self-attention mechanism without trailable weights\n",
    "\n",
    "### now calculating attention weights for all inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e0d4fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [\n",
    "    [0.43, 0.15, 0.89], # Your   (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts  (x^3)\n",
    "    [0.22, 0.58, 0.33], # with    (x^4)\n",
    "    [0.77, 0.25, 0.10], # one     (x^5)\n",
    "    [0.05, 0.80, 0.55]  # step    (x^6)\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "500f3867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de46c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "49182478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = torch.empty(inputs.shape[0], inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "  # print(f\"i: {i}\")\n",
    "  # print(f\"x_i: {x_i}\")\n",
    "  # print(\"@\")\n",
    "  for j, x_j in enumerate(inputs):\n",
    "    # print(f\"x_j: {x_j}\")\n",
    "    attn_scores[i, j] = torch.dot(x_i, x_j) \n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f119caac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# faster way to do the same thing with matrix multiplication\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7e9228aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now add the softmax to the attention scores\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "attn_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2fd7bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "inputs.T:\n",
      "tensor([[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "        [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "        [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]])\n"
     ]
    }
   ],
   "source": [
    "# this is the transposed matrix of inputs\n",
    "print(\"inputs:\")\n",
    "print(inputs)\n",
    "print(\"inputs.T:\")\n",
    "print(inputs.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "fcc58cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so two lines of code to do the same thing\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(inputs @ inputs.T, dim=1)\n",
    "\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "18a58ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now calculate the context vector\n",
    "context_vec = attn_weights @ inputs\n",
    "\n",
    "print(context_vec.shape)\n",
    "context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9544a88",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8471f0",
   "metadata": {},
   "source": [
    "## Implementing self-attention with **trainable weights**\n",
    "\n",
    "### Computing the attention weights step by step\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8dd71",
   "metadata": {},
   "source": [
    "![Self Attention Weights Diagram](./images/self-attention-weights.png.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23b24f",
   "metadata": {},
   "source": [
    "In Chapter 3, the transition from \"simple self-attention\" to \"scaled dot-product attention\" involves the introduction of **trainable weight matrices** ($W_q, W_k, W_v$). These matrices are used to transform your input tokens ($x$) into three distinct representational spaces: **Query ($q$)**, **Key ($k$)**, and **Value ($v$)** 1\\.  \n",
    "Here is an explanation of what these values represent and how they work:\n",
    "\n",
    "#### 1\\. The Trainable Weight Matrices\n",
    "\n",
    "Instead of using the raw input vector ($x$) for every calculation, the model uses three separate weight matrices:\n",
    "\n",
    "* **$W_q$ (Query weights):** Multiplied by the input to create the Query vector ($q$) 2\\.  \n",
    "* **$W_k$ (Key weights):** Multiplied by the input to create the Key vector ($k$) 2\\.  \n",
    "* **$W_v$ (Value weights):** Multiplied by the input to create the Value vector ($v$) 2\\.\n",
    "\n",
    "These matrices are **parameters** that start as random numbers and are optimized during training 3, 4\\. This allows the model to learn how to \"re-interpret\" the same input word differently depending on whether it is acting as a query, a key, or a piece of content 5\\.\n",
    "\n",
    "#### 2\\. The Three Vectors ($q, k, v$)\n",
    "\n",
    "The sources explain these terms using an **information retrieval (database)** analogy 6:\n",
    "\n",
    "* **Query ($q$):** This represents the **current token** the model is trying to understand 6\\. It acts like a **search query** you type into a database to find relevant information from other parts of the sentence 6\\.  \n",
    "* **Key ($k$):** This acts like a **database index** or a label 7\\. Every token in the sequence provides a Key that is compared against the current Query. The more a Key matches a Query (calculated via dot product), the more \"attention\" the model pays to that token 7, 8\\.  \n",
    "* **Value ($v$):** This represents the **actual content** or information of the token 7\\. Once the model uses the Query and Key to decide which tokens are important, it extracts and sums up the \"Values\" of those important tokens to create the final context vector 7, 9\\.\n",
    "\n",
    "#### 3\\. How they interact in the math\n",
    "\n",
    "The process follows these specific steps:\n",
    "\n",
    "1. **Transformation:** You multiply your input $x$ by $W_q, W_k, W_v$ to get your $q, k, v$ vectors 2\\.  \n",
    "2. **Scoring:** You take the **dot product of your Query ($q$) and all Keys ($k$)** to see how well they match 8, 10\\.  \n",
    "3. **Scaling and Normalization:** You scale these scores (dividing by the square root of the dimension) and apply **softmax** to turn them into percentages (attention weights) 11\\.  \n",
    "4. **Weighted Sum:** You multiply those percentages by the **Value ($v$)** vectors and sum them up 9\\. This ensures the final \"context vector\" is made up of the most relevant information 12, 13\\.\n",
    "\n",
    "**Analogy:** Imagine you are a **researcher (the Query)** looking for information about \"climate change\" in a **library**.\n",
    "\n",
    "* The **library's digital catalog (the Keys)** contains titles and keywords for every book. You compare your query to these titles to see which books are relevant.  \n",
    "* The **actual information inside the books (the Values)** is what you actually read and take notes from.  \n",
    "* Your **final research paper (the Context Vector)** is a \"weighted sum\" of all the information you gathered, where you took 80% of your notes from the most relevant books and only 2% from the less relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c4ef7299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1ba5bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b9f3e5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.9724, -0.7550],\n",
       "        [ 0.3239, -0.1085],\n",
       "        [ 0.2103, -0.3908]], requires_grad=True)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# torch.nn.Parameter is a wrapper around a tensor that allows it to be trainable \n",
    "W_query = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a2faf99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4796, -0.5166],\n",
       "        [-0.3107,  0.2057],\n",
       "        [ 0.9657,  0.7057]], requires_grad=True)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_key = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "W_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "062a165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.9601, -0.4087],\n",
       "        [ 1.0764, -0.4015],\n",
       "        [-0.7291, -0.1218]], requires_grad=True)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_value = torch.nn.Parameter(torch.randn(d_in, d_out))\n",
    "W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "0fc0f79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1729, -0.0048], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3ad267b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1823, -0.6888],\n",
       "        [-0.1142, -0.7676],\n",
       "        [-0.1443, -0.7728],\n",
       "        [ 0.0434, -0.3580],\n",
       "        [-0.6467, -0.6476],\n",
       "        [ 0.3262, -0.3395]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "a099b227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5526, -0.7627],\n",
       "        [-0.5401,  0.4147],\n",
       "        [-0.5412,  0.3956],\n",
       "        [-0.2483,  0.4370],\n",
       "        [-0.4085, -0.0622],\n",
       "        [-0.2599,  0.6266]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = inputs @ W_value\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d7360ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1142, -0.7676], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attention scores\n",
    "keys_2 = keys[1]\n",
    "keys_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e40d2583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1376, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score_22 = torch.dot(query_2, keys_2)\n",
    "attention_score_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "60317844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_2 = query_2 @ keys.T\n",
    "attention_scores_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2e3e29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1704, 0.1611, 0.1652, 0.1412, 0.2505, 0.1117],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "print(d_k)\n",
    "\n",
    "\n",
    "attn_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim=-1) # ** 0.5 is the square root of the dimension of the keys\n",
    "attn_weights_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2a019b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "7ac4c6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4370,  0.1182], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(values.shape)\n",
    "# print(values)\n",
    "# print(attn_weights_2.shape)\n",
    "#print(attn_weights_2)\n",
    "\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a410f",
   "metadata": {},
   "source": [
    "## Implementing a compact Self-Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e6f733ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2845, 0.4071],\n",
       "        [0.2854, 0.4081],\n",
       "        [0.2854, 0.4075],\n",
       "        [0.2864, 0.3974],\n",
       "        [0.2863, 0.3910],\n",
       "        [0.2860, 0.4039]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generalize it and get the context vector for all the inputs at once\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "  def __init__(self, d_in, d_out):\n",
    "    super().__init__()\n",
    "    self.W_query = torch.nn.Parameter(torch.randn(d_in, d_out)) # trainable weights\n",
    "    self.W_key = torch.nn.Parameter(torch.randn(d_in, d_out)) # trainable weights\n",
    "    self.W_value = torch.nn.Parameter(torch.randn(d_in, d_out)) # trainable weights\n",
    "\n",
    "  def forward(self, x):\n",
    "    queries = x @ self.W_query\n",
    "    keys = x @ self.W_key\n",
    "    values = x @ self.W_value\n",
    "\n",
    "    attn_scores = queries @ keys.T\n",
    "    attn_weights = torch.softmax(attn_scores / d_k ** 0.5, dim=-1) # ** 0.5 is the square root of the dimension of the keys\n",
    "    context_vec = attn_weights @ values\n",
    "\n",
    "    return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "self_attn_v1 = SelfAttention_v1(d_in, d_out)\n",
    "\n",
    "self_attn_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "8110be04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5980, -0.2029],\n",
       "        [-0.4980,  0.0467],\n",
       "        [-0.1320, -0.3793]], requires_grad=True)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Linear(2, 3)\n",
    "m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0aecc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize the self-attention class above\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "  def forward(self, x):\n",
    "    queries = self.W_query(x)\n",
    "    keys = self.W_key(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    attn_scores = queries @ keys.T\n",
    "    attn_weights = torch.softmax(attn_scores / d_k ** 0.5, dim=-1) # ** 0.5 is the square root of the dimension of the keys\n",
    "    context_vec = attn_weights @ values\n",
    "\n",
    "    return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "self_attn_v2 = SelfAttention_v2(d_in, d_out)\n",
    "self_attn_v2(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974983f",
   "metadata": {},
   "source": [
    "## Hiding future words with causal attention\n",
    "\n",
    "### Applying causal attention mask\n",
    "\n",
    "Basically hiding the unknown words \"Your -> journey\" in the first step, hide the other 4 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "29496ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
       "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
       "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = self_attn_v2.W_query(inputs)\n",
    "keys = self_attn_v2.W_key(inputs)\n",
    "values = self_attn_v2.W_value(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / d_k ** 0.5, dim=-1) # ** 0.5 is the square root of the dimension of the keys\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "f8944016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = attn_weights.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "mask_simple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "67680230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_masked_simple = attn_weights * mask_simple\n",
    "attn_weights_masked_simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "55f0917f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now normalize the weights again (the don't sum to 1 anymore)\n",
    "rows_sum = attn_weights_masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = attn_weights_masked_simple / rows_sum\n",
    "masked_simple_norm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305684a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
       "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
       "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
       "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's do the same in less steps\n",
    "\n",
    "mask = torch.tril(torch.ones(inputs.shape[0], inputs.shape[0]))\n",
    "masked = attn_scores.masked_fill(mask == 0, float(\"-inf\")) # -inf squared is 0\n",
    "masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc983c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now normalize the weights\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1) # keys.shape[-1] is the dimension of the keys and the same as d_k\n",
    "attn_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "9b5545cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 0.],\n",
       "        [0., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 0., 2.],\n",
       "        [0., 2., 0., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0.]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now add a dropout layer - though with newer LLMs it is not used anymore\n",
    "torch.manual_seed(123)\n",
    "\n",
    "layer = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "layer(example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000a262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.5085, 0.4936, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3906, 0.0000],\n",
       "        [0.3249, 0.3418, 0.0000, 0.3308, 0.3249, 0.3363]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(attn_weights) # scales the remaining values to compensate for the dropped out values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153d8a1",
   "metadata": {},
   "source": [
    "## Implementing a compact causal self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "9c3b77de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c96c7ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack([inputs, inputs], dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "87043480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7976, -0.2622],\n",
       "         [-0.8226, -0.2565],\n",
       "         [-0.3206, -0.1053],\n",
       "         [-0.9066, -0.3096],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [    nan,     nan]],\n",
       "\n",
       "        [[-0.9584, -0.2871],\n",
       "         [-0.5867, -0.1790],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [-0.4799, -0.1576],\n",
       "         [-0.8427, -0.3002],\n",
       "         [    nan,     nan]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class CausalAttention(nn.Module):\n",
    "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.dropout = torch.nn.Dropout(dropout)\n",
    "    self.register_buffer(\"mask\", torch.tril(torch.ones(context_length, context_length))) # register buffer because otherwise it would not be moved to the GPU\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, num_tokens, d_in = x.shape\n",
    "    # x = batch, 2 x 6 x 3\n",
    "    queries = self.W_query(x)\n",
    "    keys = self.W_key(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    attn_scores = queries @ keys.transpose(1, 2)\n",
    "    attn_scores.masked_fill_( # the underscore means in place operation = no copy of the tensor is made\n",
    "      self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # :num_tokens to account for cases where the context length is not the same for all the inputs\n",
    "    attn_weights = torch.softmax(\n",
    "      attn_scores / keys.shape[-1] ** 0.5, dim=-1) # ** 0.5 is the square root of the dimension of the keys\n",
    "    attn_weights = self.dropout(attn_weights)\n",
    "    context_vec = attn_weights @ values\n",
    "\n",
    "    return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "dropout = 0.5\n",
    "context_length = batch.shape[1] # batch.shape[1] = torch.Size([2, 6, 3]) -> 6\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout)\n",
    "ca(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad801ca",
   "metadata": {},
   "source": [
    "## Extending single-head attention to multi-head attention \n",
    "\n",
    "### Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13232faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7675, -0.2244,  0.2713,  0.1995],\n",
       "         [-0.3961, -0.1508,  0.6994,  0.5711],\n",
       "         [-0.5729, -0.2084,  0.2476,  0.2284],\n",
       "         [ 0.0000,  0.0000,  0.8587,  0.6483],\n",
       "         [-0.8427, -0.3002,  0.7671,  0.7078],\n",
       "         [    nan,     nan,     nan,     nan]],\n",
       "\n",
       "        [[-0.6067, -0.1995,  0.1965,  0.1226],\n",
       "         [-0.4463, -0.1524,  0.5849,  0.4014],\n",
       "         [-0.8586, -0.3102,  0.2476,  0.2284],\n",
       "         [-0.4799, -0.1576,  0.4816,  0.3004],\n",
       "         [ 0.0000,  0.0000,  0.7671,  0.7078],\n",
       "         [    nan,     nan,     nan,     nan]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_in, d_out, dropout, num_heads=2, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([\n",
    "      CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads) # not optimal because for loop is slow and not parallelized in GPU\n",
    "    ])\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    return out\n",
    "\n",
    "torch.manual_seed(123)\n",
    "dropout = 0.5\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "num_heads = 2\n",
    "\n",
    "multi_head_attn = MultiHeadAttention(d_in, d_out, dropout, num_heads)\n",
    "multi_head_attn(batch)\n",
    "\n",
    "## Implementing a compact multi-head self-attention class\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4c9f9",
   "metadata": {},
   "source": [
    "### Implementing multi-head attention with wieght splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f134fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "    self.d_out = d_out\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_out // num_heads\n",
    "    \n",
    "    self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.out_proj = torch.nn.Linear(d_out, d_out) # Linear layer to combine the head outputs\n",
    "    self.dropout = torch.nn.Dropout(dropout)\n",
    "    self.register_buffer(\"mask\", torch.tril(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, num_tokens, d_in = x.shape\n",
    "    q = self.W_query(x)\n",
    "    k = self.W_key(x)\n",
    "    v = self.W_value(x)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
